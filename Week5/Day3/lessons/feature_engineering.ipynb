{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20d632e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Person  Color_Blue  Color_Green  Color_Red\n",
      "0       1       False        False       True\n",
      "1       2        True        False      False\n",
      "2       3       False         True      False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample dataset\n",
    "data = {'Person': [1, 2, 3],\n",
    "        'Favorite Car Color': ['Red', 'Blue', 'Green']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df_encoded = pd.get_dummies(df, columns=['Favorite Car Color'], prefix=['Color'])\n",
    "\n",
    "# Display the encoded dataset\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69ce3756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'Weight (grams)': [150, 130, 160, 140, 155, 145],\n",
    "        'Color (1=Red, 0=Orange)': [1, 1, 0, 0, 1, 0],\n",
    "        'Fruit Type': ['Apple', 'Apple', 'Orange', 'Orange', 'Apple', 'Orange']}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87476826",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Weight (grams)', 'Color (1=Red, 0=Orange)']]\n",
    "y = df['Fruit Type']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6cc012",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe173eb",
   "metadata": {},
   "source": [
    "Gradient Boosting is like a process where you start with a simple guess, find out where you went wrong, and then make a slightly better guess based on your mistakes. You keep doing this until your guesses are really good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71a3a6f",
   "metadata": {},
   "source": [
    "When to Use:\n",
    "\n",
    "Gradient Boosting is suitable for a wide range of classification tasks.\n",
    "- Particularly effective when dealing with structured data.\n",
    "- It builds decision trees sequentially, improving on mistakes made by the previous trees.\n",
    "- Useful for reducing bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10f9d853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Gradient Boosting): 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create and train the Gradient Boosting classifier\n",
    "gb_model = GradientBoostingClassifier()\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "gb_y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "gb_accuracy = accuracy_score(y_test, gb_y_pred)\n",
    "print(f'Accuracy (Gradient Boosting): {gb_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda29f7c",
   "metadata": {},
   "source": [
    "### XGBoost (Extreme Gradient Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da37bf53",
   "metadata": {},
   "source": [
    "XGBoost is a popular choice in machine learning competitions and real-world applications because of its speed and accuracy. It’s like taking Gradient Boosting to the next level by using advanced techniques to learn from errors and make very accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7949a1a5",
   "metadata": {},
   "source": [
    "When to Use:\n",
    "\n",
    "XGBoost is a powerful and versatile model suitable for a wide range of classification tasks.\n",
    "- It works well when dealing with structured data, tabular data, and numeric features.\n",
    "- Especially effective when dealing with imbalanced datasets.\n",
    "- Popular in Kaggle competitions due to its high performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f656ccb",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee725f50",
   "metadata": {},
   "source": [
    "Random Forest is known for its simplicity, speed, and ability to handle complex data. It’s like having a group of diverse advisors (trees) to help you make better decisions (predictions) in machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1f621b",
   "metadata": {},
   "source": [
    "When to Use:\n",
    "\n",
    "- Random Forest is robust and versatile, suitable for various classification tasks.\n",
    "- Effective for both structured and unstructured data.\n",
    "- Handles high-dimensional data well.\n",
    "- Resistant to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fee2d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Random Forest): 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create and train the Random Forest classifier\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "rf_y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
    "print(f'Accuracy (Random Forest): {rf_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e973f73",
   "metadata": {},
   "source": [
    "### Support Vector Classifier (SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0beb923",
   "metadata": {},
   "source": [
    "SVC is known for its ability to handle complex datasets and find the most effective way to separate different classes. It’s like finding the safest path through a minefield by drawing a well-placed fence to guide you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6553e433",
   "metadata": {},
   "source": [
    "When to Use:\n",
    "\n",
    "- SVC is useful for binary classification tasks.\n",
    "- Works well when there’s a clear margin of separation between classes.\n",
    "- Effective for high-dimensional data.\n",
    "- Sensitive to feature scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee9e3d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (SVC): 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create and train the Support Vector Classifier\n",
    "svc_model = SVC()\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "svc_y_pred = svc_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "svc_accuracy = accuracy_score(y_test, svc_y_pred)\n",
    "print(f'Accuracy (SVC): {svc_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
